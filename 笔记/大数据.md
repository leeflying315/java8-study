# 分布式设计

CAP是一个分布式系统设计的定理，他包含3个部分，并且最多只能同时满足其中两个。

1. Consistency一致性，因为在一个分布式系统中，数据肯定需要在不同的节点之间进行同步，就比如Zookeeper，所以一致性就是指的是数据在不同的节点之间怎样保证一致性，对于纯理论的C而言，默认的规则是忽略掉延迟的，因为如果考虑延迟的话，因为数据同步的过程无论如何都会有延迟的，延迟的过程必然会带来数据的不一致。
2. Availability可用性，这个指的是对于每一个请求，节点总是可以在合理的时间返回合理的响应，比如Zookeeper在进行数据同步时，无法对外提供读写服务，不满足可用性要求。这里常有的一个例子是说Zookeeper选举期间无法提供服务不满足A，这个说法并不准确，因为CAP关注的是数据的读写，选举可以认为不在考虑范围之内。所以，可以认为对于数据的读写，无论响应超时还是返回异常都可以认为是不满足A。
3. Partition-tolerance分区容错性，因为在一个分布式系统当中，很有可能由于部分节点的网络问题导致整个集群之间的网络不连通，所以就产生了网络分区，整个集群的环境被分隔成不同的的子网，所以，一般说网络不可能100%的不产生问题，所以P一定会存在。

# ODS设计

人们对数据的处理行为可以划分为操作型数据处理和分析型数据处理，操作型数据处理一般放在传统的**数据库**(`Database,DB`)中进行，分析型数据处理则需要在**数据仓库**(`Data Warehouse,DW`)中进行。但是并不是所有的数据处理都可以这样划分，换句话说，人们对数据的处理需求并不只有这两类，比如，有些操作型处理并不适合放在传统的数据库上完成，也有些分析型处理不适合在数据仓库中进行。这时候就需要第三种数据存储体系，**操作数据存储**(`Operational Data Store,ODS`)系统就因此产生。它的出现，也将`DB~DW`两层数据架构转变成`DB~ODS~DW`三层数据架构。

> **那么，什么是ODS?**
> ODS是用于支持企业日常的、全局应用的数据集合。

ODS中的数据具有以下4个基本特征：
① **面向主题的：**进入ODS的数据是来源于各个操作型数据库以及其他外部数据源，数据进入ODS前必须经过 `ETL`过程（抽取、清洗、转换、加载等）。
② **集成的：**ODS的数据来源于各个操作型数据库，同时也会在数据清理加工后进行一定程度的综合。
③ **可更新的：**可以联机修改。这一点区别于数据仓库。
④ **当前或接近当前的：**“当前”是指数据在存取时刻是最新的，“接近当前”是指存取的数据是最近一段时间得到的。
ODS是这样一种数据存储系统，它将来自不同数据源的数据（各种操作型数据库、外部数据源等）通过ETL过程汇聚整合成面向主题的、集成的、企业全局的、一致的数据集合（主要是最新的或者最近的细节数据以及可能需要的汇总数据），用于满足企业准实时的OLAP操作和企业全局的OLTP操作，并为数据仓库提供集成后的数据，将数据仓库系统中的ETL过程下沉到ODS中完成以减轻数据仓库的压力。

# Kafka

![Kafka模型](.\pic\Kafka模型.jpg)

## Kafka rebalance 

`Rebalance` 发生时，`Group` 下所有 `Consumer` 实例都会协调在一起共同参与，`Kafka` 能够保证尽量达到最公平的分配。但是 `Rebalance` 过程对 `Consumer Group` 会造成比较严重的影响。在 `Rebalance` 的过程中 `Consumer Group` 下的所有消费者实例都会停止工作，等待 `Rebalance` 过程完成。

**发生 rebalance 的时机**

1. 组成员个数发生变化。例如有新的 `consumer` 实例加入该消费组或者离开组。
2. 订阅的 `Topic` 个数发生变化。
3. 订阅 `Topic` 的分区数发生变化。
4. coordinator挂了，集群选举出新的coordinator
5. consumer调用unsubscrible()，取消topic的订阅

如果使用了 static membership 功能后，触发 rebalance 的条件如下：

- 新成员加入组：这个条件依然不变。当有新成员加入时肯定会触发 Rebalance 重新分配分区
- Leader 成员重新加入组：比如主题分配方案发生变更
- 现有成员离组时间超过了 `session.timeout.ms` 超时时间：即使它是静态成员，Coordinator 也不会无限期地等待它。一旦超过了 session 超时时间依然会触发 Rebalance
- Coordinator 接收到 LeaveGroup 请求：成员主动通知 Coordinator 永久离组

Kafka2.4之后新增

- Incremental Rebalance Protocol：改进了eager协议，避免STW发生
- static membership：避免重起或暂时离开的消费者触发重平衡

**cooperative协议将一次全局重平衡，改成每次小规模重平衡，直至最终收敛平衡的过程**。

假设有这样一种场景，一个topic有三个分区，分别是p1，p2，p3。有两个消费者c1，c2在消费这三个分区，{c1 -> p1, p2}，{c2 -> p3}。

当然这样说不平衡的，所以加入一个消费者c3，此时触发重平衡。我们先列出在eager协议的框架下会执行的大致步骤，然后再列出cooperative发生的步骤，以做比对。

**eager 协议版本**
先说下各个名词：

- group coordinator：重平衡协调器，负责处理重平衡生命周期中的各种事件
- hearbeat：consumer和broker的心跳，重平衡时会通过这个心跳通知信息
- join group request：consumer客户端加入组的请求
- sync group request：重平衡后期，group coordinator向consumer客户端发送的分配方案

如果在 eager 版本中，会发生如下事情。

1. 最开始的时候，c1 c2 各自发送hearbeat心跳信息给到group coordinator（负责重平衡的协调器）
2. 这时候group coordinator收到一个join group的request请求，group coordinator知道有新成员加入组了
3. 在下一个心跳中group coordinator 通知 c1 和 c2 ，准备rebalance
4. **c1 和 c2 放弃（revoke）各自的partition**，然后发送joingroup的request给group coordinator
5. group coordinator处理好分配方案（交给leader consumer分配的），发送sync group request给 c1 c2 c3，附带新的分配方案
6. c1 c2 c3接收到分配方案后，重新开始消费

如果在cooperative版本中，会发生如下事情。

1. 最开始的时候c1 c2各自发送hearbeat心跳信息给到group coordinator
2. 这时候group coordinator收到一个join group的request请求，group coordinator知道有新成员加入组了
3. 在下一个心跳中 group coordinator 通知 c1 和 c2 ，准备 rebalance，前面几部分是一样的
4. **c1 和 c2发送joingroup的request给group coordinator，但不需要revoke其所拥有的partition，而是将其拥有的分区编码后一并发送给group coordinator，即 {c1->p1, p2}，{c2->p3}**
5. group coordinator 从元数据中获取当前的分区信息（这个称为assigned-partitions），再从c1 c2 的joingroup request中获取分配的分区（这个称为 owned-partitions），通过assigned-partitions和owned-partitions知晓当前分配情况，决定取消c1一个分区p2的消费权，然后发送sync group request（{c1->p1}，{c2->p3}）给c1 c2，让它们继续消费p1 p2
6. c1 c2 接收到分配方案后，重新开始消费，一次 rebalance 完成，**当然这时候p2处于无人消费状态**
7. 再次触发rebalance，重复上述流程，不过这次的目的是把p2分配给c3（通过assigned-partitions和owned-partitions获取分区分配状态）

cooperative协议版重平衡的一个核心，是assigned-partitions和owned-partitions，group coordinator通过这两者，可以保存和获取分区的消费状态，以便进行多次重平衡并达到最终的均衡状态。

**Kafka性能秘诀**

Kafka在磁盘上只做Sequence I/O。

- Sequence I/O: 600MB/s
- Random I/O: 100KB/s

Kafka重度依赖底层操作系统提供的PageCache功能

传统的网络I/O操作流程，大体上分为以下4步：

1. OS 从硬盘把数据读到内核区的PageCache。
2. 用户进程把数据从内核区Copy到用户区。
3. 然后用户进程再把数据写入到Socket，数据流入内核区的Socket Buffer上。
4. OS 再把数据从Buffer中Copy到网卡的Buffer上，这样完成一次发送。

![](.\pic\传统IO.jpg)

整个过程共经历两次Context Switch，四次System Call。同一份数据在内核Buffer与用户Buffer之间重复拷贝，效率低下。其中2、3两步没有必要，完全可以直接在内核区完成数据拷贝。这也正是Sendfile所解决的问题，经过Sendfile优化后，整个I/O过程就变成了下面这个样子。

![](.\pic\零拷贝技术.jpg)

## Kafka为什么这么快

1. partition顺序读写，充分利用磁盘特性，这是基础；
2. Producer生产的数据持久化到broker，采用mmap文件映射，实现顺序的快速写入
3. Customer从broker读取数据，采用sendfile，将磁盘文件读到OS内核缓冲区后，直接转到socket buffer进行网络发送

### **mmap 和 sendfile总结**

1. 都是Linux内核提供、实现零拷贝的API；
2. sendfile 是将读到内核空间的数据，转到socket buffer，进行网络发送；
3. mmap将磁盘文件映射到内存，支持读和写，对内存的操作会反映在磁盘文件上。
   RocketMQ 在消费消息时，使用了 mmap。kafka 使用了 sendFile。

**消费者进程挂掉的情况**

1. `session` 过期
2. `heartbeat` 过期

## Kafka中ZK的作用

Kafka使用ZooKeeper存放集群元数据、成员管理、Controller选举，以及其他一些管理类任务。之后，等KIP-500提案完成后，Kafka将完全不再依赖于ZooKeeper。

- “存放元数据”是指主题分区的所有数据都保存在 ZooKeeper 中，且以它保存的数据为权威，其他 “人” 都要与它保持对齐。
- “成员管理” 是指 Broker 节点的注册、注销以及属性变更，等等。
- “Controller 选举” 是指选举集群 Controller，而其他管理类任务包括但不限于主题删除、参数配置等。

## Kafka 零拷贝

在Kafka中，体现Zero Copy使用场景的地方有两处：基于mmap的索引和日志文件读写所用的TransportLayer。

- 索引都是基于MappedByteBuffer的，也就是让用户态和内核态共享内核态的数据缓冲区，此时，数据不需要复制到用户态空间。不过，mmap虽然避免了不必要的拷贝，但不一定就能保证很高的性能。在不同的操作系统下，mmap的创建和销毁成本可能是不一样的。很高的创建和销毁开销会抵消Zero Copy带来的性能优势。由于这种不确定性，**在Kafka中，只有索引应用了mmap**，最核心的日志并未使用mmap机制。
- TransportLayer是Kafka传输层的接口。它的某个实现类使用了FileChannel的transferTo方法。**该方法底层使用sendfile实现了Zero Copy**。对Kafka而言，如果I/O通道使用普通的PLAINTEXT，那么，Kafka就可以利用Zero Copy特性，直接将页缓存中的数据发送到网卡的Buffer中，避免中间的多次拷贝。相反，如果I/O通道启用了SSL，那么，Kafka便无法利用Zero Copy特性了。

传统IO拷贝过程：

- 1）发出read系统调用，会导致用户空间到内核空间的上下文切换，然后再通过DMA将文件中的数据从磁盘上读取到内核空间缓冲区
- 2）接着将内核空间缓冲区的数据拷贝到用户空间进程内存，然后read系统调用返回。而系统调用的返回又会导致一次内核空间到用户空间的上下文切换
- 3）write系统调用，则再次导致用户空间到内核空间的上下文切换，将用户空间的进程里的内存数据复制到内核空间的socket缓冲区（也是内核缓冲区，不过是给socket使用的），然后write系统调用返回，再次触发上下文切换
- 4）至于socket缓冲区到网卡的数据传输则是独立异步的过程，也就是说write系统调用的返回并不保证数据被传输到网卡

「一共有四次用户空间与内核空间的上下文切换。四次数据copy，分别是两次CPU数据复制，两次DMA数据复制」

![](..\笔记\pic\java8\传统IO传输文件.png)

### mmap+write实现的零拷贝

![](..\笔记\pic\java8\namp+write零拷贝.png)

1. 发出mmap系统调用，导致用户空间到内核空间的上下文切换。然后通过DMA引擎将磁盘文件中的数据复制到内核空间缓冲区
2. mmap系统调用返回，导致内核空间到用户空间的上下文切换
3. 这里不需要将数据从内核空间复制到用户空间，因为用户空间和内核空间共享了这个缓冲区
4. 发出write系统调用，导致用户空间到内核空间的上下文切换。将数据从内核空间缓冲区复制到内核空间socket缓冲区；write系统调用返回，导致内核空间到用户空间的上下文切换
5. 异步，DMA引擎将socket缓冲区中的数据copy到网卡

「通过mmap实现的零拷贝I/O进行了4次用户空间与内核空间的上下文切换，以及3次数据拷贝；其中3次数据拷贝中包括了2次DMA拷贝和1次CPU拷贝」

### sendfile实现的零拷贝.

![](..\笔记\pic\java8\sendFile.png)

- 1）发出sendfile系统调用，导致用户空间到内核空间的上下文切换，然后通过DMA引擎将磁盘文件中的内容复制到内核空间缓冲区中，接着再将数据从内核空间缓冲区复制到socket相关的缓冲区
- 2）sendfile系统调用返回，导致内核空间到用户空间的上下文切换。DMA异步将内核空间socket缓冲区中的数据传递到网卡

「通过sendfile实现的零拷贝I/O使用了2次用户空间与内核空间的上下文切换，以及3次数据的拷贝。其中3次数据拷贝中包括了2次DMA拷贝和1次CPU拷贝」

### 带有DMA收集拷贝功能的sendfile实现的零拷贝

![](..\笔记\pic\java8\dma收集拷贝.png)

- 1）发出sendfile系统调用，导致用户空间到内核空间的上下文切换。通过DMA引擎将磁盘文件中的内容复制到内核空间缓冲区
- 2）这里没把数据复制到socket缓冲区；取而代之的是，相应的描述符信息被复制到socket缓冲区。该描述符包含了两种的信息：A)内核缓冲区的内存地址、B)内核缓冲区的偏移量
- 3）sendfile系统调用返回，导致内核空间到用户空间的上下文切换。DMA根据socket缓冲区的描述符提供的地址和偏移量直接将内核缓冲区中的数据复制到网卡

「带有DMA收集拷贝功能的sendfile实现的I/O使用了2次用户空间与内核空间的上下文切换，以及2次数据的拷贝，而且这2次的数据拷贝都是非CPU拷贝。这样一来我们就实现了最理想的零拷贝I/O传输了，不需要任何一次的CPU拷贝，以及最少的上下文切换

## Kafka消费单线程

Java Consumer是双线程的设计。一个线程是用户主线程，负责获取消息；另一个线程是心跳线程，负责向Kafka汇报消费者存活情况。将心跳单独放入专属的线程，能够有效地规避因消息处理速度慢而被视为下线的“假死”情况。

单线程获取消息的设计能够避免阻塞式的消息获取方式。单线程轮询方式容易实现异步非阻塞式，这样便于将消费者扩展成支持实时流处理的操作算子。因为很多实时流处理操作算子都不能是阻塞式的。另外一个可能的好处是，可以简化代码的开发。多线程交互的代码是非常容易出错的。

## 简述Follower副本消息同步的完整流程

首先，Follower发送FETCH请求给Leader。

接着，Leader会读取底层日志文件中的消息数据，再更新它内存中的Follower副本的LEO值，更新为FETCH请求中的fetchOffset值。

最后，尝试更新分区高水位值。Follower接收到FETCH响应之后，会把消息写入到底层日志，接着更新LEO和HW值。

Leader和Follower的HW值更新时机是不同的，Follower的HW更新永远落后于Leader的HW。这种时间上的错配是造成各种不一致的原因。

因此，对于消费者而言，消费到的消息永远是所有副本中最小的那个HW。



- LEO（Log End Offset）：日志末端位移值或末端偏移量，表示日志下一条待插入消息的位移值。举个例子，如果日志有10条消息，位移值从0开始，那么，第10条消息的位移值就是9。此时，LEO = 10。
- LSO（Log Stable Offset）：这是Kafka事务的概念。如果你没有使用到事务，那么这个值不存在（其实也不是不存在，只是设置成一个无意义的值）。该值控制了事务型消费者能够看到的消息范围。它经常与Log Start Offset，即日志起始位移值相混淆，因为有些人将后者缩写成LSO，这是不对的。在Kafka中，LSO就是指代Log Stable Offset。
- AR（Assigned Replicas）：AR是主题被创建后，分区创建时被分配的副本集合，副本个数由副本因子决定。
- ISR（In-Sync Replicas）：Kafka中特别重要的概念，指代的是AR中那些与Leader保持同步的副本集合。在AR中的副本可能不在ISR中，但Leader副本天然就包含在ISR中。**所有与leader副本保持一定程度同步的副本（包括Leader）组成ISR（In-Sync Replicas）。**ISR集合是AR集合中的一个子集。
- HW（High watermark）：高水位值，这是控制消费者可读取消息范围的重要字段。一个普通消费者只能“看到”Leader副本上介于Log Start Offset和HW（不含）之间的所有消息。水位以上的消息是对消费者不可见的。
- OSR：与leader副本同步滞后过多的副本（不包括leader）副本，组成OSR(Out-Sync Relipcas),由此可见：AR=ISR+OSR

需要注意的是，通常在ISR中，可能会有人问到为什么有时候副本不在ISR中，这其实也就是上面说的Leader和Follower不同步的情况，为什么我们前面说，短暂的不同步我们可以关注，但是长时间的不同步，我们需要介入排查了，因为ISR里的副本后面都是通过replica.lag.time.max.ms，即Follower副本的LEO落后Leader LEO的时间是否超过阈值来决定副本是否在ISR内部的。

isr-expiration任务会周期性的检测每个分区是否需要缩减其ISR集合。这个周期和“replica.lag.time.max.ms”参数有关。大小是这个参数一半。默认值为5000ms，当检测到ISR中有是失效的副本的时候，就会缩减ISR集合。如果某个分区的ISR集合发生变更， 则会将变更后的数据记录到ZooKerper对应/brokers/topics/partition/state节点中

节点中数据示例如下：

{“controller_cpoch":26,“leader”:0,“version”:1,“leader_epoch”:2,“isr”:{0,1}}

 其中controller_epoch表示的是当前的kafka控制器epoch.leader表示当前分区的leader副本所在的broker的id编号，version表示版本号，（当前半本固定位1），leader_epoch表示当前分区的leader纪元，isr表示变更后的isr列表。

Kafka的副本策略称为ISRs(in-sync replicas)，动态维护了一个包含所有已提交日志的节点集合，通过zookeeper存储该集合，并由zookeeper从集合中选出一个节点作为leader，日志会先写入到leader，再由ISRs中的其他follower节点主动进行复制同步。

Quorum算法是为了解决脑裂问题，而ISRs这里不会出现这个问题的原因是zookeeper本身是一个分布式协调服务，可以通过zookeeper保证leader的唯一性。

kafka副本策略另外一个设计是每次日志写入并不会进行fsync等刷盘操作，刷盘会导致两到三倍的性能损失。崩溃的节点恢复后并不一定拥有完整的数据，但是可以通过和leader重新同步来加入ISRs。

### Kafka分区策略

### 分区策略

> 1. 多Partition分布式存储，利于集群数据的均衡。
> 2. 并发读写，加快读写速度。
> 3. 加快数据恢复的速率：当某台机器挂了，每个Topic仅需恢复一部分的数据，多机器并发。

**分区的原则**

> 1. 指明partition的情况下，使用指定的partition；
> 2. 没有指明partition，但是有key的情况下，将key的hash值与topic的partition数进行取余得到partition值；
> 3. 既没有指定partition，也没有key的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与topic可用的partition数取余得到partition值，也就是常说的round-robin算法。

## Kafka 集群Metadata管理

### 存放在哪里

- 在每个Broker的`KafkaServer`对象中都会创建`MetadataCache`组件, 负责缓存所有的metadata信息;

```text
val metadataCache: MetadataCache = new MetadataCache(config.brokerId)
```

- 所在文件: core/src/main/scala/kafka/server/MetadataCache.scala
- 所有的metadata信息存储在map里, key是topic, value又是一个map, 其中key是parition id, value是`PartitionStateInfo`

```text
private val cache: mutable.Map[String, mutable.Map[Int, PartitionStateInfo]] =
    new mutable.HashMap[String, mutable.Map[Int, PartitionStateInfo]]()
```

- `PartitionStateInfo`: 包括`LeaderIsrAndControllerEpoch`和Replica数组; 下面的`readFrom`方法从接受到的buffer构造一个`PartitionStateInfo`对象:

```text
def readFrom(buffer: ByteBuffer): PartitionStateInfo = {
    val controllerEpoch = buffer.getInt
    val leader = buffer.getInt
    val leaderEpoch = buffer.getInt
    val isrSize = buffer.getInt
    val isr = for(i <- 0 until isrSize) yield buffer.getInt
    val zkVersion = buffer.getInt
    val replicationFactor = buffer.getInt
    val replicas = for(i <- 0 until replicationFactor) yield buffer.getInt
    PartitionStateInfo(LeaderIsrAndControllerEpoch(LeaderAndIsr(leader, leaderEpoch, isr.toList, zkVersion), controllerEpoch),
                       replicas.toSet)
  }
```

- `MetadataCache`还保存着推送过来的有效的broker信息

```text
private var aliveBrokers: Map[Int, Broker] = Map()
```

### MetadataCache如何获取和更新metadata信息

**干三件事** 1. 更新`aliveBrokers`; 2. 如果某个topic的的parition的leader是无效的, 则`removePartitionInfo(tp.topic, tp.partition)`; 3. 新增或更新某个topic的某个parition的信息, `addOrUpdatePartitionInfo(tp.topic, tp.partition, info)`: 将信息meta信息保存到`MetadataCache`的`cache`对象中;

## Metadata信息从哪里来

- 这个问题实际上就是在问`UpdateMetaRequest`是*谁*在*什么时候*发送的;
- 来源肯定是`KafkaController`发送的;
- broker变动, topic创建, partition增加等等时机都需要更新metadata;

![](.\pic\java8\心跳.png)

Kafka内部状态流转

## Kafka 的coordinator

### 1. 什么是coordinator？

每个consumer group 都会选择一个**broker**作为自己的coordinator，**他是负责监控整个消费组里的各个分区的心跳，以及判断是否宕机，和开启rebalance的**。

集群中的每一个 broker 节点在启动时都会创建并启动一个 GroupCoordinator 实例，每个实例都会管理集群中所有消费者 group 的一个子集。

### 2. 如何选择coordinator机器

首先对group id 进行hash，接着对`__consumer_offsets`的分区数量进行取模，默认分区数量是50

`__consumer_offsets`的分区数量可以通过offsets.topic.num.partitions来设置，找到分区以后，这个分区所在的broker机器就是coordinator机器。

**eg：**

group id = “group1”

“group1” .hash() % 50 =5

这个时候我们得到了5，那么我们就去看看这个8号分区在那一台机器，然后我们这个group1下的所有消费者就知道了提交offset的时候是往哪个分区去提交offset

### 3. 消费流程

1. 每个consumer都会发送JoinGroup请求到我们刚才计算出来的coordinator那台机器和是哪个
2. 然后coordinator从一个consumer group 中取出一个consumer作为leader
3. coordinator把consumer group 情况发送给这个leader
4. 接着leader会负责制定消费方案
5. 通过SyncGroup发送给coordinator
6. 接着coordinator就把消费方案下发给所有的consumer，他们会从指定的分区的leader broker开始进行socket连接和进行消息的消费

![](.\pic\java8\心跳.png)

## Kafka Fetch和Poll关系

**max.partition.fetch.bytes为每个分片最大拉取的数据量大小。**

**max.poll.records 变量限制了每次 poll 的消息条数，不管 consumer 对应多少个 partition，从所有 partition 拉取到的消息条数总和不会超过 `maxPollRecords`**

**在满足max.partition.fetch.bytes限制的情况下，假如fetch到了100个record，放到本地缓存后，由于max.poll.records限制每次只能poll出15个record。那么KafkaConsumer就需要执行7次才能将这一次通过网络发起的fetch请求所fetch到的这100个record消费完毕。其中前6次是每次pool中15个record，最后一次是poll出10个record。**

首先，Kafka在[1.1.0](https://issues.apache.org/jira/browse/KAFKA-6254)以后的版本中优化了Fetch问题，引入了Fetch Session，Kafka由Broker来提供服务（通信、数据交互等）。每个分区会有一个Leader Broker，Broker会定期向Leader Broker发送Fetch请求，来获取数据，而对于分区数较大的Topic来说，需要发出的Fetch请求就会很大。这样会有一个问题：

- Follower感兴趣的分区集很少改变，然而每个FetchRequest必须枚举Follower感兴趣的所有分区集合；
- **当上一个FetchRequest只会分区中没有任何改变，仍然必须发回关于该分区的所有元数据，其中包括分区ID、分区的起始Offset、以及能够请求的最大字节数等。**

并且，这些问题与系统中现存分区的数量成线性比例，例如，假设Kafka集群中有100000个分区，其中大多数分区很少接收新消息。该系统中的Broker仍然会来回发送非常大的FetchRequest和FetchResponse，即使每秒添加的实际消息数据很少。随着分区数量的增长，Kafka使用越来越多的网络带宽来回传递这些消息。

当Kafka被调整为较低延迟时，这些低效会变得更严重。如果我们将每秒发送的FetchRequest数量增加一倍，我们应该期望在缩短的轮询间隔内有更多的分区没有改变。而且，我们无法在每个FetchRequest和FetchResponse中分摊每个分区发送元数据的所需要的带宽资源，这将会导致Kafka需要使用更多的网络带宽。

为了优化上述问题，Kafka增加了增量拉取分区的概念，从而减少客户端每次拉取都需要拉取全部分区的问题。Fetch Session与网络编程中的Session类似，可以认为它是有状态的，这里的状态值的是知道它需要拉取哪些分区的数据，比如第一次拉取的分区0中的数据，后续分区0中没有了数据，就不需要拉取分区0了，FetchSession数据结构如下

```java
class FetchSession(val id: Int, // sessionid是随机32位数字，用于鉴权，防止客户端伪造
                   val privileged: Boolean, // 是否授权
                   val partitionMap: FetchSession.CACHE_MAP,// 缓存数据CachedPartitionMap
                   val creationMs: Long, // 创建Session的时间
                   var lastUsedMs: Long, // 上次使用会话的时间，由FetchSessionCache更新
                   var epoch: Int) // 获取会话序列号
```

因为Fetch Session使用的是Leader上的内存，所以我们需要限制在任何给定时间内的内存量，因此，每个Broker将只创建有限数量的增量Fetch Session。以下，有两个公共参数，用来配置Fetch Session的缓存：

- max.incremental.fetch.session.cache.slots：用来限制每台Broker上最大Fetch Session数量，默认1000
- min.incremental.fetch.session.eviction.ms：从缓存中逐步增量获取会话之前等待的最短时间，默认120000

当服务器收到创建增量Fetch Session请求时，它会将新的Session与先有的Session进行比较，只有在下列情况下，新Session才会有效：

- 新Session在Follower里面；
- 现有Session已停止，且超过最小等待时间；
- 现有Session已停止，且超过最小等待时间，并且新Session有更多的分区。

这样可以实现如下目标：

- Follower优先级高于消费者；
- 随着时间的推移，非活跃的Session将被替换；
- 大请求（从增量中收益更多）被优先处理；
- 缓存抖动是有限的，避免了昂贵的Session重建时。

## Kafka Leader 选举器

触发重平衡由失衡率(imbalanceRatio)决定:
ratio = Sum(partitonsNotLeaded)/Sum(partitonsShouldLeaded)%
当ratio大于*leader.imbalance.per.broker.percentage*(默认10%)时会触发重平衡

在多replica的场景下，消息的读写都是通过leader来完成，其他replica则是通过从leader读取数据来完成消息的同步以保证leader异常时消息的完善性。 然而在多个replica的场景下，谁能成为leader？ leader异常后，又如何选出新的leader呢这是kafka提供可靠服务的关键所在。

可能触发为partition选举leader的场景有: 

- 新创建topic
- broker启动
- broker停止
- controller选举
- 客户端触发
- reblance等等

在不同的场景下选举方法不尽相同。Kafka提供了五种leader选举方式，继承PartitionLeaderSelector，实现selectLeader方法完成leader的选举， 下面对这五种leader选举方式给予说明：

1. NoOpLeaderSelector： 该选举器不会调用到，大概是拿来测试用的。 该选举器直接返回当前的leader Irs，AR。

2. OfflinePartitionLeaderSelector：

   ```text
   触发场景：
       * 新创建topic
       * PartitionStateMachine启动
       * broker启动时
       * ReplicaStateMachine检测到broker的znode“被删除”
   选举：
       1） Isr列表中有存货的replica，直接选出
       2） 否则，unclean.leader.election.enable 为false，抛出异常
       3） 存活的ar中有replica，选出，否则抛出异常
   ```

3. ControlledShutdownLeaderSelector

   ```undefined
   触发场景：
       * kafka的broker进程退出发送消息给controller，controller触发
   选举：
       * 在isr列表中的选出存活的replica，否则抛出异常
   ```

4. PreferredReplicaPartitionLeaderSelector

```swift
触发场景：
    * znode节点/admin/preferred_replica_election写入相关数据
    * partition-rebalance-thread线程进行触发reblance时
	* 新产生controller
选举 ：
    1） AR中取出一个作为leader，如果与原有leader一样，抛出异常
    2） 新leade的replica的broker存活且replica在isr中，选出，否则抛出异常
```

5. ReassignedPartitionLeaderSelector

```undefined
触发场景:
    * znode节点LeaderAndIsr发生变化
    * Broker启动时
    * zknode节点/admin/reassign_partitions变动
    * 新产生controller时
选举：
    * 新设置的ar中，存在broker存活的replica且replica在isr中则选出为leader，否则抛出异常
```

## Kafka 2.5.0启动步骤

1. 判断状态是否满足可启动（不在停止过程中，没被其他服务调用启动中）
2. 连接Zookeeper
3. 在ZK上生成或者获取唯一ClusterID(根据UUID)
4. 载入MetaData
5. 

# ClickHouse

## 数据类型

### 基本类型

只有数值，字符串，时间。没有 `Boolean` ，但可以通过 `0/1` 来替代。

| 基本类型              | 对应 Mysql          |
| --------------------- | ------------------- |
| `Int8`                | `Tinyint`           |
| `Int16`               | `Smallint`          |
| `Int32`               | `Int`               |
| `Int64`               | `Bigint`            |
| `Float32`             | `Float`             |
| `Float64`             | `Double`            |
| `String`              | `Varchar Text Blob` |
| `FixedString(N)`      | `Char`              |
| `UUID [8-4-4-4-12]`   | 无                  |
| `DateTime [到秒]`     |                     |
| `DateTime64 [到亚秒]` |                     |
| `Date [到天]`         |                     |





MergeTree、ReplacingMergeTree、CollapsingMergeTree、VersionedCollapsingMergeTree、SummingMergeTree、AggregatingMergeTree引擎。

ClickHouse 优势：

- 数据剪枝能力强分区剪枝在执行层，而存储格式用局部数据表示，就可以更细粒度地做一些数据的剪枝。它的引擎在实际使用中应用了一种现在比较流行的 LSM 方式。
- 它对整个资源的垂直整合能力做得比较好，并发 MPP+ SMP 这种执行方式可以很充分地利用机器的集成资源。它的实现又做了很多性能相关的优化，它的一个简单的汇聚操作有很多不同的版本，会根据不同 Key 的组合方式有不同的实现。对于高级的计算指令，数据解压时，它也有少量使用。
- ClickHouse  是一套完全由 C++ 模板 Code 写出来的实现，代码还是比较优雅的。
- ClickHouse 是一个完全的列式数据库

ClickHouse用做分析引擎

Druid 和 ES。ES 不适合大批量数据的查询，Druid 则不满足明细数据查询的需求。而 ClickHouse 则刚好适合这个场景。

### MergeTree

MergeTree虽然有主键索引，但是其主要作用是加速查询，而不是类似MySQL等数据库用来保持记录唯一。即便在Compaction完成后，主键相同的数据行也仍旧共同存在。

### ReplacingMergeTree

ReplacingMergeTree可以对主键去重，但是存在缺点：

- **在没有彻底optimize之前，可能无法达到主键去重的效果，比如部分数据已经被去重，而另外一部分数据仍旧有主键重复**；
- **在分布式场景下，相同primary key的数据可能被sharding到不同节点上，不同shard间可能无法去重**；
- optimize是后台动作，无法预测具体执行时间点；
- 手动执行optimize在海量数据场景下要消耗大量时间，无法满足业务即时查询的需求；

因此ReplacingMergeTree更多被用于确保数据最终被去重，而无法保证查询过程中主键不重复。

### CollapsingMergeTree

ClickHouse实现了CollapsingMergeTree来消除ReplacingMergeTree的限制。该引擎要求在建表语句中指定一个标记列Sign，后台Compaction时会将主键相同、Sign相反的行进行折叠，也即删除。

CollapsingMergeTree将行按照Sign的值分为两类：Sign=1的行称之为状态行，Sign=-1的行称之为取消行。

每次需要新增状态时，写入一行状态行；需要删除状态时，则写入一行取消行。

在后台Compaction时，状态行与取消行会自动做折叠（删除）处理。而尚未进行Compaction的数据，状态行与取消行同时存在。

因此为了能够达到主键折叠（删除）的目的，需要业务层进行适当改造：

1） 执行删除操作需要写入取消行，而取消行中需要包含与原始状态行一样的数据（Sign列除外）。所以在应用层需要记录原始状态行的值，或者在执行删除操作前先查询数据库获取原始状态行；

2）由于后台Compaction时机无法预测，在发起查询时，状态行和取消行可能尚未被折叠；另外，ClickHouse无法保证primary key相同的行落在同一个节点上，不在同一节点上的数据无法折叠。因此在进行count(*)、sum(col)等聚合计算时，可能会存在数据冗余的情况。为了获得正确结果，业务层需要改写SQL，将`count()、sum(col)`分别改写为`sum(Sign)、sum(col * Sign)`。

CollapsingMergeTree虽然解决了主键相同的数据即时删除的问题，但是状态持续变化且多线程并行写入情况下，状态行与取消行位置可能乱序，导致无法正常折叠。

### VersionedCollapsingMergeTree

VersionedCollapsingMergeTree表引擎在建表语句中新增了一列Version，用于在乱序情况下记录状态行与取消行的对应关系。主键相同，且Version相同、Sign相反的行，在Compaction时会被删除。

### SummingMergeTree

ClickHouse通过SummingMergeTree来支持对主键列进行预先聚合。在后台Compaction时，会将主键相同的多行进行sum求和，然后使用一行数据取而代之，从而大幅度降低存储空间占用，提升聚合计算性能。

- ClickHouse只在后台Compaction时才会进行数据的预先聚合，而compaction的执行时机无法预测，所以可能存在部分数据已经被预先聚合、部分数据尚未被聚合的情况。因此，在执行聚合计算时，SQL中仍需要使用GROUP BY子句。
- 在预先聚合时，ClickHouse会对主键列之外的其他所有列进行预聚合。如果这些列是可聚合的（比如数值类型），则直接sum；如果不可聚合（比如String类型），则随机选择一个值。
- 通常建议将SummingMergeTree与MergeTree配合使用，使用MergeTree来存储具体明细，使用SummingMergeTree来存储预先聚合的结果加速查询。



### AggregatingMergeTree

## Flink

优势：

- 整合了实时和离线计算为一套框架。
- 低延迟、高吞吐，保证正确性

spark streaming 高吞吐、正确性，但是延迟高。

storm 延迟低，但是性能和高并发下正确性不能保证。

运行时组件： 

**JobManager**

**TaskManager**：执行task。内存隔离，每一部分为一个Slot。注册到resourceManager。

**ResourceManager**：管理分配slot资源。

**Dispacher**： 分发任务，提供界面。



# Zookeeper

## Zookeeper集群架构

![zookeeper](.\pic\java8\zookeeper.jpg)

Zookeeper中通常只有**Leader节点可以写入，Follower和Observer都只是负责读，但是Follower会参与节点的选举**和**过半写成功**，Observer则不会，他只是单纯的提供读取数据的功能。这样做的缺点是读操作可能会返回过时的数据，但提高了读的性能。

ZooKeeper 有两个基本的一致性保证：**线性写和先进先出(FIFO)的客户端请求**。

ZooKeeper 通过 `zxid` 来实现， `zxid` 是最后一个事务的标记，当客户端发出一个请求到一个相同或者不同的副本时，会在请求带上 `zxid` 标记，副本通过检查客户端的 `zxid` 和自己的 `zxid` ，保证读到的是更新的 `zxid` 的数据(没有具体说怎么处理，是阻塞等待还是拒绝请求)。

**数据节点Znode**

Zookeeper中数据存储于内存之中，这个数据节点就叫做Znode，他是一个树形结构，比如/a/b/c类似。

而Znode又分为持久节点、临时节点、顺序节点三大类。

持久节点是指只要被创建，除非主动移除，否则都应该一直保存在Zookeeper中。

临时节点不同的是，他的生命周期和客户端Session会话一样，会话失效，那么临时节点就会被移除。

还有就是临时顺序节点和持久顺序节点，除了基本的特性之外，子节点的名称还具有有序性。

**会话Session**

会话自然就是指Zookeeper客户端和服务端之间的通信，他们使用TCP长连接的方式保持通信，通常，肯定会有心跳检测的机制，同时他可以接受来自服务器的Watch事件通知。

**事件监听器Wather**

用户可以在指定的节点上注册Wather，这样在事件触发的时候，客户端就会收到来自服务端的通知。

**权限控制ACL**

Zookeeper使用ACL来进行权限的控制，包含以下5种：

1. CREATE，创建子节点权限
2. DELETE，删除子节点权限
3. READ，获取节点数据和子节点列表权限
4. WRITE，更新节点权限
5. ADMIN，设置节点ACL权限

所以，Zookeeper通过集群的方式来做到高可用，通过内存数据节点Znode来达到高性能，但是存储的数据量不能太大，通常适用于读多写少的场景。

### Zookeeper有哪些应用场景？

1. 命名服务Name Service，依赖Zookeeper可以生成全局唯一的节点ID，来对分布式系统中的资源进行管理。
2. 分布式协调，这是Zookeeper的核心使用了。利用Wather的监听机制，一个系统的某个节点状态发生改变，另外系统可以得到通知。
3. 集群管理，分布式集群中状态的监控和管理，使用Zookeeper来存储。
4. Master选举，利用Zookeeper节点的全局唯一性，同时只有一个客户端能够创建成功的特点，可以作为Master选举使用，创建成功的则作为Master。
5. 分布式锁，利用Zookeeper创建临时顺序节点的特性。

### Zookeeper是如何保证数据一致性的？

Zookeeper通过ZAB原子广播协议来实现数据的最终顺序一致性，他是一个类似2PC两阶段提交的过程。

由于Zookeeper只有Leader节点可以写入数据，如果是其他节点收到写入数据的请求，则会将之转发给Leader节点。

主要流程如下：

1. Leader收到请求之后，将它转换为一个proposal提议，并且为每个提议分配一个全局唯一递增的事务ID：zxid，然后把提议放入到一个FIFO的队列中，按照FIFO的策略发送给所有的Follower
2. Follower收到提议之后，以事务日志的形式写入到本地磁盘中，写入成功后返回ACK给Leader
3. Leader在收到超过半数的Follower的ACK之后，即可认为数据写入成功，就会发送commit命令给Follower告诉他们可以提交proposal了

![](..\笔记\pic\java8\zookeeper数据一致性.jpg)

ZAB包含两种基本模式，崩溃恢复和消息广播。

整个集群服务在启动、网络中断或者重启等异常情况的时候，首先会进入到崩溃恢复状态，此时会通过选举产生Leader节点，当集群过半的节点都和Leader状态同步之后，ZAB就会退出恢复模式。之后，就会进入消息广播的模式。



## Zookeeper 选举过程

1. 集群模式下，每个人先投自己一票。然后把自己的选票通知到其他所有节点。
2. 其他节点收到选票后会和自身myId比较，如果自己的MyId比较大。

- 竞选 Leader 看的是 epoch、写请求操作数、myid 三个字段，依次比较谁大谁就更有资格成为 Leader
- 获选超过半数以上的办事处正式成为 Leader，修改自己状态为 LEADING
- 其他 Participant 修改为 FOLLOWING，Observer 则修改为 OBSERVING
- 如果集群中已经存在一个 Leader，其他办事处如果中途加入的话，直接跟随该 Leader 即可
- 还得提一句，如果当前可提供服务的节点已经不足半数以上了，那么这个选举就永远无法选出结果，每个节点都会一直处在 LOOKING 状态，整个办事处集群也就无法对外提供服务了。

选举 Leader 完全看的就是这几个值

- epoch
- 写请求次数
- myid

优先级从上到下逐级比较，谁大谁就更有资格成为 Leader，当前级一样就比较下一级，直到分出胜负为止！因为 myid 是不能重复的，所以最终是一定能分出胜负的！

选票主要有这些信息：

- sid：我是谁
- leader：我选谁
- state：我当前的状态
- epoch：我当前的 epoch
- zxid：我选择的 leader 的最大的事务编号